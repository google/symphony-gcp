{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d55fb4",
   "metadata": {},
   "source": [
    "# Symphony HostFactory GKE Plug-in Testing Simplified Data Treatment\n",
    "\n",
    "This notebook is meant to walkthrough the data sourcing, data treatment and visualization for the instrumentation developped in the `bootstrap-gke` project. \n",
    "\n",
    "At the end of the document, we shall have a simple dataset containing:\n",
    "\n",
    "- Time for which a GKE nodes are ready\n",
    "- Time for which pods are scheduled\n",
    "- Allocated vCPU accross time\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "In this simplified version of analysis, we require only two data sources:\n",
    "\n",
    "- **HostFactory API**:\n",
    "    - Data from this source is manually noted by the user.\n",
    "    - This simplified analysis requires only the request time (i.e. when the user requests the HostFactory provider plugin).\n",
    "    - Optionally, the user can also note the machine types and/or templates and their respective quantity. \n",
    "- **Cloud Logging Pipeline**:\n",
    "    - Data from this source is ingested automatically into a BigQuery table.\n",
    "    - Data is ingested through logging sinks and forwarded to a pub/sub topic, with a respective subscription pointed to BigQuery, with a small UDF transformation. \n",
    "    - Schema for this BigQuery table can be found at the `automate/instances/orchestrator/base/bigquery-schemas/logs.json` path.\n",
    "    - A complete list of the logging sinks definitions can be found at `automate/instances/bootstrap-gke/observability/sinks`. Please note that these sinks are Terraform templates which are formated in function of the configurations of the bootstraping terraform project.\n",
    "    - A complete list of the UDF transforms can be found at the `automate/instances/bootstrap-gke/observability/transforms` folder. These sinks are also Terraform templates which also are formated. \n",
    "    - Following is a list of the file names for the logging sinks which interest us for this analysis:\n",
    "        - `node_ready_patch.lql.tftpl` - Event logged when a node is upgraded to Ready state.\n",
    "        - `pod_scheduled.lql.tftpl` - Event logged when a pod is scheduled onto a node.\n",
    "- **Cloud Monitoring**:\n",
    "    - The vCPU usage of the cluster is available through Cloud Monitoring. Alternativelly this can be deduced from the nodes ready events from the previous data-source. \n",
    "\n",
    "## Data Treatment \n",
    "\n",
    "Data treatment starts by defining the variables we want to query from the BigQuery database. For this, we are required to set the following variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943637f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query by run id\n",
    "PROJECT_ID = \"symphony-dev-2\"\n",
    "DATASET_ID = \"log_dataset_default\"\n",
    "TABLE_ID = \"logs-2\"\n",
    "\n",
    "# When the request was made, as noted by the experiment metadata\n",
    "REQUEST_TIMESTAMP = \"2025-10-01T09:08:58+00:00\"\n",
    "RETURN_TIMESTAMP = None # In this example, we forcefully delete pods and do not analyse scale-down.\n",
    "\n",
    "# The run ID, as configured by the tester\n",
    "RUN_ID = \"test-bigscale-14\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde4af6",
   "metadata": {},
   "source": [
    "Following, let us query the data which interest us for this simple analysis. For this, we will also import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set maximum width for table view\n",
    "pd.set_option('max_colwidth', 60)\n",
    "# Set maximum rows for table view\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "client = bigquery.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform the request timestamp from string to a datetime objet at UTC.\n",
    "REQUEST_TIMESTAMP = datetime.datetime.fromisoformat(REQUEST_TIMESTAMP).astimezone(datetime.UTC)\n",
    "\n",
    "QUERY = \"\"\"\n",
    "SELECT time, event, node, pod\n",
    "from `{project}.{dataset}.{table}`\n",
    "WHERE (\n",
    "    run = \"{run_id}\" AND\n",
    "    time > \"{start_time}\" AND \n",
    "    time < \"{end_time}\" AND\n",
    "    event IN (\"node:ready_patch\", \"pod:scheduled\")\n",
    ")\n",
    "\"\"\".format(\n",
    "    project = PROJECT_ID,\n",
    "    dataset = DATASET_ID,\n",
    "    table = TABLE_ID,\n",
    "    run_id = RUN_ID,\n",
    "    start_time = REQUEST_TIMESTAMP.isoformat(),\n",
    "    # We query up to 20 minutes after the start of the test\n",
    "    end_time = (REQUEST_TIMESTAMP + datetime.timedelta(minutes=20)).isoformat()\n",
    ")\n",
    "\n",
    "# with open(DATA_FOLDER.joinpath(\"QUERY.sql\"), \"w\") as fh:\n",
    "#     fh.writelines(QUERY)\n",
    "\n",
    "print(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5299b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()\n",
    "raw_df = rows.to_dataframe()\n",
    "\n",
    "# Optionally save the data for future reference\n",
    "# df.to_parquet(DATA_FOLDER.joinpath(f\"{RUN_ID}.parquet\"))\n",
    "\n",
    "raw_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d799910",
   "metadata": {},
   "source": [
    "For analysis, we will pivot the table to have nodes as indexes, since we are interested on the first pod scheduled event for each node, and the last ready patch event for each node. \n",
    "\n",
    "To achieve this, we'll use two aggregation functions (\"first\" and \"last\"), and latter get our only desired datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cdc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy(deep=True)\n",
    "\n",
    "df = df.pivot_table(\n",
    "    values=\"time\",\n",
    "    index=\"node\",\n",
    "    columns=\"event\",\n",
    "    aggfunc=[\"first\",\"last\"]\n",
    ")\n",
    "\n",
    "# Join column multi index into index\n",
    "columns = [\":\".join(column) for column in df.columns]\n",
    "df.columns = pd.Index(columns)\n",
    "\n",
    "# Drop unnused events\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"first:node:ready_patch\",\n",
    "        \"last:pod:scheduled\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Total lines in DataFrame: {len(df.index)}\")\n",
    "\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc7fe3",
   "metadata": {},
   "source": [
    "Since the only `pod:scheduled` events ingested are for the (Symphony) workload pods, we can drop the rows (nodes), which don't have a value in this columns. These nodes correspond to system or operator nodes / node pools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77167d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"first:pod:scheduled\"].isna()]\n",
    "\n",
    "print(f\"Total lines in DataFrame: {len(df.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d03b5a",
   "metadata": {},
   "source": [
    "We will now extract the number of cores and machine family from the node name, since this respects our framework naming pattern we are able to deduce the number of cores for each node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37498767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "machine_config = {}\n",
    "\n",
    "for machine in df.index:\n",
    "    match = re.match(\n",
    "        \"gke-cluster-test-0-(.*)-(.*)-node-pool-te-.*\",\n",
    "        machine\n",
    "    )\n",
    "    if match is None:\n",
    "        raise Exception(\"Failed to match machine config.\")\n",
    "    groups = match.groups()\n",
    "    machine_config[machine] = {\n",
    "        \"machine:family\": groups[0],\n",
    "        \"machine:cores\": int(groups[1])\n",
    "    }\n",
    "\n",
    "machine_config = pd.DataFrame.from_dict(machine_config, orient=\"index\")\n",
    "df = df.join(machine_config)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8cae0",
   "metadata": {},
   "source": [
    "Now, we have all the data we require for our plot. It is simply not adapted for visualization. The last step is to format the data such that it is plotable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f49a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us sort by the time the first pod was scheduled\n",
    "pod_count = (\n",
    "    df[\"first:pod:scheduled\"]\n",
    "        .reset_index()\n",
    "        .set_index(\"first:pod:scheduled\")\n",
    "        .sort_index()\n",
    ")\n",
    "\n",
    "# This is equivalent to counting the number of rows,\n",
    "# or more precisely, generating a column coresponding to the \n",
    "# count index of the first pod scheduled events\n",
    "pod_count[\"pod:count\"] = range(1, len(pod_count.index) + 1)\n",
    "\n",
    "# We remove the node names, as we do not plot this\n",
    "pod_count = pod_count.drop(columns=[\"node\"])\n",
    "\n",
    "pod_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3aa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same for the nodes, but add the cummulative sum of the cores\n",
    "node_count = (\n",
    "    df[[\"last:node:ready_patch\", \"machine:cores\"]]\n",
    "        .reset_index()\n",
    "        .set_index(\"last:node:ready_patch\")\n",
    "        .sort_index()\n",
    ")\n",
    "\n",
    "# Equivalent to the count index of nodes\n",
    "node_count[\"node:count\"] = range(1, len(node_count.index) + 1)\n",
    "\n",
    "# We generate the cummulative sum of the cores\n",
    "node_count[\"node:cores_sum\"] = node_count[\"machine:cores\"].cumsum()\n",
    "\n",
    "# Remove node name and individual cores values\n",
    "node_count = node_count.drop(columns=[\"node\", \"machine:cores\"])\n",
    "\n",
    "node_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113b303",
   "metadata": {},
   "source": [
    "Now we have our data which is plotable, the remaining code is plotting formatting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "parsed_data = pd.concat([\n",
    "    pod_count,\n",
    "    node_count\n",
    "])\n",
    "\n",
    "# Artifically add the zero index\n",
    "parsed_data = pd.concat([\n",
    "    pd.DataFrame([[0,0,0]], index=[REQUEST_TIMESTAMP], columns=parsed_data.columns),\n",
    "    parsed_data\n",
    "])\n",
    "\n",
    "parsed_data = parsed_data.rename(columns={\n",
    "    \"node:count\": \"GkeNumberOfNodes\",\n",
    "    \"node:cores_sum\": \"GkeNumberOfCores\",\n",
    "    \"pod:count\": \"NumberOfSymphonyPods\"\n",
    "})\n",
    "# parsed_data.index.name = \"Timestamp\"\n",
    "\n",
    "# Optionally reset index to REQUEST_TIMESTAMP\n",
    "parsed_data.index = (parsed_data.index - REQUEST_TIMESTAMP).total_seconds()\n",
    "parsed_data.index.name = \"TimeAfterSymphonyRequest\"\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=parsed_data.index,\n",
    "        y=parsed_data.GkeNumberOfNodes,\n",
    "        mode=\"lines\",\n",
    "        name=\"GKE - Number of nodes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=parsed_data.index,\n",
    "        y=parsed_data.GkeNumberOfCores,\n",
    "        mode=\"lines\",\n",
    "        name=\"GKE - Number of cores\"\n",
    "    ),\n",
    "    secondary_y=True\n",
    "   \n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=parsed_data.index,\n",
    "        y=parsed_data.NumberOfSymphonyPods,\n",
    "        mode=\"lines\",\n",
    "        name=\"Number of Symphony Pods\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Scaling performance of IBM Spectrum Symphony connector for GKE\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    legend=dict(\n",
    "        x=0.005,\n",
    "        y=0.95,\n",
    "        bordercolor='black',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    # xaxis_range=[\n",
    "    #     REQUEST_TIMESTAMP,\n",
    "    #     REQUEST_TIMESTAMP + datetime.timedelta(minutes=10)\n",
    "    # ],\n",
    "    xaxis_range=[0,600]\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_xaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey',\n",
    "    title_text=\"Time after Symphony HostFactory GKE plugin request\", \n",
    "    tickvals=list(range(60,660,60)),\n",
    "    ticktext=[f\"{x} min\" for x in range(1,11)],\n",
    "    # tickangle=45\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey',\n",
    "    tickcolor=\"lightgrey\",\n",
    "    zerolinecolor='lightgrey',\n",
    "    title_text=\"Number of Pods and Nodes\" \n",
    "\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=False,\n",
    "    showgrid=False,\n",
    "    linecolor=None,\n",
    "    gridcolor=None,\n",
    "    title_text=\"Number of Cores\",\n",
    "    secondary_y=True,   \n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    connectgaps=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679dfcf",
   "metadata": {},
   "source": [
    "Finally, optinally save all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7616baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(f\"/home/user/data-{RUN_ID}\")\n",
    "\n",
    "# Save the metadata\n",
    "with open(DATA_FOLDER.joinpath(\"metadata.json\"), \"w\") as fh:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"PROJECT_ID\": PROJECT_ID,\n",
    "            \"DATASET_ID\": DATASET_ID,\n",
    "            \"TABLE_ID\": TABLE_ID,\n",
    "            \"REQUEST_TIMESTAMP\": REQUEST_TIMESTAMP.isoformat() if REQUEST_TIMESTAMP is not None else None,\n",
    "            \"RETURN_TIMESTAMP\": RETURN_TIMESTAMP.isoformat() if RETURN_TIMESTAMP is not None else None,\n",
    "            \"RUN_ID\": RUN_ID\n",
    "        },\n",
    "        fh,\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "# Save the corresponding query\n",
    "with open(DATA_FOLDER.joinpath(\"QUERY.sql\"), \"w\") as fh:\n",
    "    fh.writelines(QUERY)\n",
    "\n",
    "# Save the corresponding raw data\n",
    "raw_df.to_parquet(DATA_FOLDER.joinpath(f\"{RUN_ID}-raw.parquet\"))\n",
    "\n",
    "# Save the corresponding indexed data\n",
    "df.to_csv(DATA_FOLDER.joinpath(f\"{RUN_ID}-indexed.csv\"))\n",
    "\n",
    "# Save the corresaponding plot data\n",
    "parsed_data.to_csv(DATA_FOLDER.joinpath(f\"{RUN_ID}-sparse.csv\"))\n",
    "\n",
    "# Save the plot itself\n",
    "with open(DATA_FOLDER.joinpath(f\"{RUN_ID}.html\"), \"w\") as fh:\n",
    "    fig.write_html(fh)\n",
    "\n",
    "fig.write_image(\n",
    "    file=DATA_FOLDER.joinpath(f\"{RUN_ID}.png\"),\n",
    "    format=\"png\",\n",
    "    width=800,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.write_image(\n",
    "    file=DATA_FOLDER.joinpath(f\"{RUN_ID}.svg\"),\n",
    "    format=\"svg\",\n",
    "    width=800,\n",
    "    height=500,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1256819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import src.table_stats\n",
    "reload(src.table_stats)\n",
    "\n",
    "from src.table_stats import print_stats\n",
    "\n",
    "# Initialize BigQuery client\n",
    "# client = bigquery.Client()\n",
    "\n",
    "# Set maximum width for table view\n",
    "pd.set_option('max_colwidth', 60)\n",
    "# Set maximum rows for table view\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "DATA_FOLDER = Path(os.getenv(\"WORKDIR\")).joinpath(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5102875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query by run id\n",
    "\n",
    "PROJECT_ID = \"symphony-dev-2\"\n",
    "DATASET_ID = \"log_dataset_default\"\n",
    "TABLE_ID = \"logs-2\"\n",
    "RUN_ID = \"test-bigscale-14\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT DISTINCT run from `{project}.{dataset}.{table}`\n",
    "\"\"\".format(\n",
    "    project = PROJECT_ID,\n",
    "    dataset = DATASET_ID,\n",
    "    table = TABLE_ID,\n",
    "    run_id = RUN_ID\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()\n",
    "df = rows.to_dataframe()\n",
    "runs = df[\"run\"].to_list()\n",
    "\n",
    "RUN_ID in runs\n",
    "# runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff01960",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT * from `{project}.{dataset}.{table}`\n",
    "WHERE run = \"{run_id}\"\n",
    "WHERE time > \"2025-10-01T10:08:58+00:00\"\n",
    "ORDER BY time DESC\n",
    "-- LIMIT 1000 \n",
    "-- Optionally limit the query when dealing with too big datasets...\n",
    "\"\"\".format(\n",
    "    project = PROJECT_ID,\n",
    "    dataset = DATASET_ID,\n",
    "    table = TABLE_ID,\n",
    "    run_id = RUN_ID\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()\n",
    "df = rows.to_dataframe()\n",
    "\n",
    "# Parse detail json string\n",
    "df.detail = df.detail.transform(lambda x: json.loads(x) if x is not None else None)\n",
    "\n",
    "# Sort by time\n",
    "df = df.set_index(\"time\").sort_index().reset_index()\n",
    "\n",
    "# Optionally identify when grr changed\n",
    "df[\"grr_shift_out\"] = df[df.event == \"cli:grr_out\"].detail != df[df.event == \"cli:grr_out\"].shift().detail\n",
    "df[\"grr_shift_in\"] = df[df.event == \"cli:grr_in\"].detail != df[df.event == \"cli:grr_in\"].shift().detail\n",
    "\n",
    "print_stats(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT COUNT(event) from `{project}.{dataset}.{table}`\n",
    "WHERE (\n",
    "    run = \"{run_id}\" AND\n",
    "    event = \"node:create\"\n",
    ")\n",
    "-- LIMIT 1000 \n",
    "-- Optionally limit the query when dealing with too big datasets...\n",
    "\"\"\".format(\n",
    "    project = PROJECT_ID,\n",
    "    dataset = DATASET_ID,\n",
    "    table = TABLE_ID,\n",
    "    run_id = RUN_ID\n",
    ")\n",
    "\n",
    "partial_query_job = client.query(QUERY)\n",
    "partial_rows = partial_query_job.result()\n",
    "partial_df = partial_rows.to_dataframe()\n",
    "\n",
    "partial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c127833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_TIMESTAMP = None; RETURN_TIMESTAMP = None\n",
    "\n",
    "REQUEST_TIMESTAMP = \"2025-10-01T09:08:58+00:00\"\n",
    "RETURN_TIMESTAMP = \"2025-09-16T13:59:08+00:00\"\n",
    "REQUEST_TIMESTAMP = datetime.datetime.fromisoformat(REQUEST_TIMESTAMP).astimezone(datetime.UTC)\n",
    "RETURN_TIMESTAMP = datetime.datetime.fromisoformat(RETURN_TIMESTAMP).astimezone(datetime.UTC)\n",
    "\n",
    "test_folder = DATA_FOLDER.joinpath(RUN_ID)\n",
    "\n",
    "cloud_hosts_path = test_folder.joinpath(\"cloud_hosts.csv\")\n",
    "\n",
    "cloud_hosts = None\n",
    "if os.path.isfile(cloud_hosts_path):\n",
    "    cloud_hosts_df = pd.read_csv(\n",
    "        cloud_hosts_path,\n",
    "        index_col=0,\n",
    "        converters={\n",
    "            \"releaseTime\": datetime.datetime.fromisoformat,\n",
    "            \"launchTime\": datetime.datetime.fromisoformat\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4de2c",
   "metadata": {},
   "source": [
    "# Parse data from systemd resource watcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = test_folder.joinpath(\"resources\")\n",
    "\n",
    "resources_dfs = None\n",
    "if os.path.isdir(resources_path):\n",
    "    resources_dfs = {\n",
    "        timestamp: pd.read_csv(\n",
    "            resources_path.joinpath(timestamp),\n",
    "        ) for timestamp in os.listdir(resources_path)\n",
    "    }\n",
    "\n",
    "def transform_maxmem(row):\n",
    "    try:\n",
    "        row.maxmem = int(row.maxmem[:-1])\n",
    "    except Exception as e:\n",
    "        print(row.status, row.maxmem)\n",
    "        raise e\n",
    "    return row\n",
    "\n",
    "def transform_status(row):\n",
    "    if '-' in row[[\n",
    "        \"maxmem\",\n",
    "        \"nprocs\",\n",
    "        \"ncores\",\n",
    "        \"nthreads\"\n",
    "    ]].to_list():\n",
    "        row.status = \"bootstraping\"\n",
    "    return row\n",
    "\n",
    "\n",
    "if resources_dfs is not None: \n",
    "    resources = []\n",
    "\n",
    "    for timestamp, resources_df in resources_dfs.items():\n",
    "        # Cathegorize items with \"-\" as bootstraping\n",
    "        resources_df = resources_df.transform(transform_status, axis=1)\n",
    "\n",
    "        # Transform maxmem to integer\n",
    "        resources_df = pd.concat([\n",
    "            resources_df.loc[~(resources_df.status == \"ok\")],\n",
    "            resources_df.loc[resources_df.status == \"ok\"].transform(transform_maxmem, axis=1)\n",
    "        ])\n",
    "\n",
    "        # Magick for extracting sum\n",
    "        resources.append({\n",
    "            \"timestamp\": datetime.datetime.fromtimestamp(int(timestamp)).astimezone(tz=datetime.UTC),\n",
    "            ** resources_df[resources_df.status == \"ok\"][[\n",
    "                \"maxmem\",\n",
    "                \"nprocs\",\n",
    "                \"ncores\",\n",
    "                \"nthreads\"\n",
    "            ]].astype(int).sum().to_dict(),\n",
    "            ** resources_df.value_counts(\"status\").to_dict()\n",
    "        })\n",
    "\n",
    "    resources = pd.DataFrame.from_records(resources)\n",
    "\n",
    "    if REQUEST_TIMESTAMP is not None:\n",
    "        resources[\"request_delta\"] = resources.timestamp.transform(lambda x: (x - REQUEST_TIMESTAMP).total_seconds())\n",
    "        resources = resources.set_index(\"request_delta\").sort_index()\n",
    "    else:\n",
    "        resources = resources.set_index(\"timestamp\").sort_index()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources.to_csv(\n",
    "    test_folder\n",
    "        .joinpath(\"resources.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d86ba2",
   "metadata": {},
   "source": [
    "# Pod Parsing and back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596377b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add preemption analysis with `dfc_pivot_preempted` and `cloud_hosts`\n",
    "\n",
    "# Copy dataframe\n",
    "dfc = df.copy(deep=True)\n",
    "\n",
    "# Filter pertinent events\n",
    "dfc = dfc[\n",
    "    dfc.event.isin([\n",
    "            \"cli:rm_out\",\n",
    "            \"pod:create\",\n",
    "            \"pod:scheduled\",\n",
    "            \"container:started\",\n",
    "            \"node:preempted\",\n",
    "            \"cli:grs_out\",\n",
    "            \"cli:rrm_in\",\n",
    "            \"pod:delete\",\n",
    "            \"scale_up\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "# ========= Parse Node Preemption =========\n",
    "\n",
    "# This transforms the `node:preempted` into multiple `pod:node_preempted` for\n",
    "# all pods that have been scheduled to the node before its preemption.\n",
    "# This is latter used to remove lines from the pivoted table\n",
    "\n",
    "def parse_preemption(row):\n",
    "    row.pod = df[\n",
    "        np.logical_and.reduce([\n",
    "            df.event == \"pod:scheduled\",\n",
    "            df.node == row.node,\n",
    "            df.time <= row.time\n",
    "        ])\n",
    "    ].pod.to_list()\n",
    "    row.event = \"pod:node_preempted\"\n",
    "    return row\n",
    "\n",
    "\n",
    "dfc = pd.concat([\n",
    "    dfc.loc[~(dfc.event == \"node:preempted\")],\n",
    "    dfc[dfc.event == \"node:preempted\"].transform(\n",
    "        parse_preemption, axis=1\n",
    "    ).explode(\"pod\")\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# ========= Parse Pod Preemption =========\n",
    "\n",
    "# Checks if the pod:delete event contains data\n",
    "# informing that the pod was deleted by the scheduler\n",
    "# due to preemption\n",
    "\n",
    "def parse_pod_preemption(row):\n",
    "    conditions = row.detail.get(\"conditions\")\n",
    "    if conditions is None:\n",
    "        return None\n",
    "    last_reason = conditions[0].get(\"reason\")\n",
    "    if last_reason is None:\n",
    "        return None\n",
    "    if last_reason == \"PreemptionByScheduler\":\n",
    "        row.event = \"pod:preempted\"\n",
    "        return row\n",
    "    return None\n",
    "\n",
    "dfc = pd.concat([   \n",
    "    dfc,\n",
    "    dfc[dfc.event == \"pod:delete\"].apply(\n",
    "        parse_pod_preemption,\n",
    "        axis=\"columns\"\n",
    "    ).dropna(how=\"all\")\n",
    "],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "# ========= Parse RM =========\n",
    "\n",
    "# This transforms a single cli:rm_out into multiple pod:rm\n",
    "# extracting the pod name\n",
    "\n",
    "def parse_rm_out(row):\n",
    "    # Get associated pods\n",
    "    row.pod = dfc[dfc.sym_request == row.detail[\"payload\"][\"requestId\"]].pod.to_list()\n",
    "    # Change event name\n",
    "    row.event = \"cli:rm\"\n",
    "    return row\n",
    "\n",
    "dfc = pd.concat([\n",
    "    dfc.loc[~(dfc.event == \"cli:rm_out\")],\n",
    "    dfc[dfc.event == \"cli:rm_out\"].transform(\n",
    "        parse_rm_out, axis=1\n",
    "    ).explode(\"pod\")\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# ========= Parse RRM =========\n",
    "\n",
    "# This transforms a single cli:rrm_in into multiple pod:rrm\n",
    "# extracting the pod name\n",
    "\n",
    "def parse_rrm_in(row):\n",
    "    # Get pod names\n",
    "    row.pod = [machine[\"name\"] for machine in row.detail[\"payload\"][\"machines\"]]\n",
    "    # Change event name\n",
    "    row.event = \"cli:rrm\"\n",
    "    return row\n",
    "\n",
    "dfc = pd.concat([\n",
    "    dfc.loc[~(dfc.event == \"cli:rrm_in\")],\n",
    "    dfc[dfc.event == \"cli:rrm_in\"].transform(\n",
    "        parse_rrm_in, axis=1\n",
    "    ).explode(\"pod\")\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# ========= Parse GRS Output =========\n",
    "\n",
    "# This extracts from the `cli:grs_out` when a machine \n",
    "# was first recognized as running by Symphony.\n",
    "\n",
    "def parse_grs_out(row):\n",
    "    # Extract pods which are running\n",
    "\n",
    "    machines = [\n",
    "        machine \n",
    "        for request in row.detail[\"payload\"][\"requests\"]\n",
    "        for machine in request[\"machines\"]\n",
    "    ]\n",
    "\n",
    "    # if len(machines) == 0:\n",
    "    #     return None\n",
    "\n",
    "    row.pod = [\n",
    "        machine[\"name\"] for machine in machines\n",
    "    ]\n",
    "\n",
    "    row.detail = [\n",
    "        machine[\"status\"] for machine in machines\n",
    "    ]\n",
    "\n",
    "    row.event = [\n",
    "        f\"cli:grs_{machine[\"result\"]}\"  for machine in machines\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "dfc = pd.concat([\n",
    "    dfc.loc[~(dfc.event == \"cli:grs_out\")],\n",
    "    dfc[dfc.event == \"cli:grs_out\"].transform(\n",
    "        parse_grs_out, axis=1\n",
    "    ).explode([\"pod\",\"detail\", \"event\"]),\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# ========= Load Cloud Hosts if existing =========\n",
    "\n",
    "if cloud_hosts is not None:\n",
    "    cloud_hosts = cloud_hosts_df.copy(deep=True)\n",
    "    cloud_hosts=cloud_hosts[[\n",
    "            \"hostname\", \"releaseTime\", \"launchTime\"\n",
    "    ]]\n",
    "    cloud_hosts.index.rename(\"pod\", inplace=True)\n",
    "    cloud_hosts.rename(\n",
    "        columns = {\n",
    "            \"launchTime\": \"hf:launched\",\n",
    "            \"releaseTime\": \"hf:released\"\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "dfc = dfc.set_index(\"time\").sort_index().reset_index()\n",
    "\n",
    "# ========= Pivot table =========\n",
    "\n",
    "# This step pivots the table to have pods as indexes, events as columns\n",
    "# and values as time. \n",
    "\n",
    "dfc_pivot = dfc.pivot_table(\n",
    "    index=\"pod\",\n",
    "    columns=\"event\",\n",
    "    values=\"time\",\n",
    "    aggfunc=[\"first\", \"last\"]\n",
    ")\n",
    "\n",
    "preempted_pods = None\n",
    "dfc_pivot_preempted = None\n",
    "# Remove preempted pod lines\n",
    "if \"pod:node_preempted\" in dfc_pivot[\"first\"].columns:\n",
    "    preempted_pods = dfc_pivot[~dfc_pivot[\"first\"][\"pod:node_preempted\"].isna()].index\n",
    "    dfc_pivot_preempted = dfc_pivot.loc[preempted_pods]\n",
    "    dfc_pivot.drop(\n",
    "        index=preempted_pods,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "preempted_pods_sched = None\n",
    "dfc_pivot_preempted_sched = None\n",
    "if \"pod:preempted\" in dfc_pivot[\"first\"].columns:\n",
    "    preempted_pods_sched = dfc_pivot[~dfc_pivot[\"first\"][\"pod:preempted\"].isna()].index\n",
    "    dfc_pivot_preempted_sched = dfc_pivot.loc[preempted_pods_sched]\n",
    "    dfc_pivot.drop(\n",
    "        index=preempted_pods_sched,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "pod_schedule = dfc_pivot.copy(deep=True)\n",
    "\n",
    "# ========= Calculate deltas =========\n",
    "\n",
    "# Scaleup Delta\n",
    "pod_scale_up = pd.DataFrame()\n",
    "pod_scale_up[\"cli:rm->pod:create\"] = (dfc_pivot[\"first\"][\"pod:create\"] - dfc_pivot[\"first\"][\"cli:rm\"]).transform(lambda x: x.total_seconds())\n",
    "if cloud_hosts is not None:\n",
    "    pod_scale_up[\"hf:launched->pod:scheduled\"] = (dfc_pivot[\"last\"][\"pod:scheduled\"] - cloud_hosts[\"hf:launched\"]).transform(lambda x: x.total_seconds())\n",
    "pod_scale_up[\"pod:create->pod:scheduled\"] = (dfc_pivot[\"first\"][\"pod:scheduled\"] - dfc_pivot[\"first\"][\"pod:create\"]).transform(lambda x: x.total_seconds())\n",
    "# pod_scale_up[\"container:started->cli:grs_succeed\"] = (dfc_pivot[\"first\"][\"cli:grs_succeed\"] - dfc_pivot[\"first\"][\"container:started\"]).transform(lambda x: x.total_seconds())\n",
    "pod_scale_up[\"pod:create->cli:grs_executing\"] = (dfc_pivot[\"first\"][\"cli:grs_executing\"] - dfc_pivot[\"first\"][\"pod:create\"]).transform(lambda x: x.total_seconds())\n",
    "pod_scale_up[\"pod:schedule->cli:grs_succeed\"] = (dfc_pivot[\"first\"][\"cli:grs_succeed\"] - dfc_pivot[\"first\"][\"pod:scheduled\"]).transform(lambda x: x.total_seconds())\n",
    "\n",
    "# Scaledown Delta\n",
    "pod_scale_down = pd.DataFrame()\n",
    "pod_scale_down[\"cli:rrm->pod:delete\"] = (dfc_pivot[\"last\"][\"pod:delete\"] - dfc_pivot[\"first\"][\"cli:rrm\"]).transform(lambda x: x.total_seconds())\n",
    "\n",
    "if cloud_hosts is not None:\n",
    "    pod_scale_down[\"pod:delete->hf:released\"] = (dfc_pivot[\"last\"][\"pod:delete\"] - cloud_hosts[\"hf:released\"]).transform(lambda x: x.total_seconds())\n",
    "\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "\n",
    "if preempted_pods is not None:\n",
    "    print(f\"Number of preempted pods: {len(preempted_pods)}\")\n",
    "\n",
    "pod_scale_down = pod_scale_down.describe(\n",
    "    percentiles=[\n",
    "        0.25, 0.5, 0.75, 0.99\n",
    "    ]\n",
    ")\n",
    "\n",
    "pod_scale_up = pod_scale_up.describe(\n",
    "    percentiles=[\n",
    "        0.25, 0.5, 0.75, 0.99\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(pod_scale_down)\n",
    "pod_scale_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc_pivot.to_csv(\n",
    "    test_folder.joinpath(\n",
    "        \"pod_events.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scale_up.rename(columns={\n",
    "    \"cli:rm->pod:create\": \"RequestMachine to Pod Created\",\n",
    "    \"pod:create->pod:scheduled\": \"Pod Created to Scheduled\"\n",
    "})[[\n",
    "    \"RequestMachine to Pod Created\",\n",
    "    \"Pod Created to Scheduled\"\n",
    "]].to_csv(\n",
    "    test_folder.joinpath(\n",
    "        \"scaleup_stats.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed4b8d",
   "metadata": {},
   "source": [
    "# Pod Scale Up Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scale_up_plot = pd.DataFrame()\n",
    "\n",
    "pod_scale_up_plot[\"cli:rm\"] = dfc_pivot[\"first\"][\"cli:rm\"]\n",
    "pod_scale_up_plot[\"pod:create\"] = dfc_pivot[\"first\"][\"pod:create\"]\n",
    "pod_scale_up_plot[\"cli:grs_executing\"] = dfc_pivot[\"first\"][\"cli:grs_executing\"]\n",
    "pod_scale_up_plot[\"pod:scheduled\"] = dfc_pivot[\"first\"][\"pod:scheduled\"]\n",
    "pod_scale_up_plot[\"cli:grs_succeed\"] = dfc_pivot[\"first\"][\"cli:grs_succeed\"]\n",
    "\n",
    "if cloud_hosts is not None:\n",
    "    pod_scale_up_plot[\"hf:launched\"] = cloud_hosts[\"hf:launched\"]\n",
    "\n",
    "pod_scale_up_plot = pod_scale_up_plot.melt().set_index(\"value\").sort_index().reset_index()\n",
    "pod_scale_up_plot[\"count\"] = pod_scale_up_plot.groupby(\"variable\").cumcount()\n",
    "scale_up_index = pod_scale_up_plot[\"value\"]\n",
    "\n",
    "pod_scale_up_plot = pod_scale_up_plot.pivot(\n",
    "    columns=\"variable\",\n",
    "    values=\"count\"\n",
    ")\n",
    "\n",
    "scale_up_events = dfc[dfc.event == \"scale_up\"].time\n",
    "\n",
    "if REQUEST_TIMESTAMP is not None:\n",
    "    pod_scale_up_plot[\"time\"] = scale_up_index.transform(\n",
    "        lambda x: (x - REQUEST_TIMESTAMP).total_seconds()\n",
    "    )\n",
    "    scale_up_events = dfc[dfc.event == \"scale_up\"].time.transform(\n",
    "        lambda x: (x - REQUEST_TIMESTAMP).total_seconds()\n",
    "    )\n",
    "else:\n",
    "    pod_scale_up_plot[\"time\"] = scale_up_index\n",
    "\n",
    "pod_scale_up_plot = pod_scale_up_plot.set_index(\"time\")\n",
    "\n",
    "pod_scale_up_plot.head()\n",
    "\n",
    "\n",
    "# ============= Configure Plot =============\n",
    "\n",
    "# Reorder columns\n",
    "pod_scale_up_plot = pod_scale_up_plot[[\n",
    "    \"cli:rm\",\n",
    "    \"pod:create\",\n",
    "    \"cli:grs_executing\",\n",
    "    \"pod:scheduled\",\n",
    "    \"cli:grs_succeed\",\n",
    "    # \"hf:launched\"\n",
    "]]\n",
    "\n",
    "pod_scale_up_plot.rename(\n",
    "    columns={\n",
    "        \"cli:rm\": \"CLI Request Machine\",\n",
    "        \"pod:create\": \"Pod Created\",\n",
    "        \"cli:grs_executing\": \"CLI Get Request Status: Executing\",\n",
    "        \"pod:scheduled\": \"Pod Scheduled\",\n",
    "        \"cli:grs_succeed\": \"CLI Get Request Status: Succeded\",\n",
    "        # \"hf:launched\": \"Host Factory API Value: Launched\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "pod_scale_up_plot.columns.rename(\"Event Count\", inplace=True)\n",
    "\n",
    "if REQUEST_TIMESTAMP is not None:\n",
    "    pod_scale_up_plot.index.rename(\"Time after HostFactory API request (seconds)\", inplace=True)\n",
    "else:\n",
    "    pod_scale_up_plot.index.rename(\"Timestamp\", inplace=True)\n",
    "\n",
    "pod_scale_up_fig = pod_scale_up_plot.plot(\n",
    "    kind=\"scatter\",\n",
    "    title=\"Pod Scale Up\"\n",
    ")\n",
    "\n",
    "for scale_up_timestamp in scale_up_events.to_list():\n",
    "    pod_scale_up_fig = pod_scale_up_fig.add_vline(\n",
    "        scale_up_timestamp,\n",
    "        line_dash=\"dash\", line_color=\"green\"\n",
    ")\n",
    "\n",
    "pod_scale_up_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scale_up_plot.to_csv(\n",
    "    test_folder.joinpath(\"pod_scale_up.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scale_up_fig.write_image(\n",
    "    file=test_folder.joinpath(\"pod_scale_up.svg\"),\n",
    "    format=\"svg\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "pod_scale_up_fig.write_html(\n",
    "    file=test_folder.joinpath(\"pod_scale_up.html\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ed255",
   "metadata": {},
   "source": [
    "# Pod Scale Down Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b02e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pod_scale_down_plot = pd.DataFrame()\n",
    "pod_scale_down_plot[\"cli:rrm\"] = dfc_pivot[\"first\"][\"cli:rrm\"]\n",
    "pod_scale_down_plot[\"pod:delete\"] = dfc_pivot[\"first\"][\"pod:delete\"]\n",
    "\n",
    "if cloud_hosts is not None:\n",
    "    pod_scale_down_plot[\"hf:released\"] = cloud_hosts[\"hf:released\"]\n",
    "\n",
    "pod_scale_down_plot = pod_scale_down_plot.melt().set_index(\"value\").sort_index().reset_index()\n",
    "pod_scale_down_plot[\"count\"] = pod_scale_down_plot.groupby(\"variable\").cumcount()\n",
    "scale_down_index = pod_scale_down_plot[\"value\"]\n",
    "\n",
    "pod_scale_down_plot = pod_scale_down_plot.pivot(\n",
    "    columns=\"variable\",\n",
    "    values=\"count\"\n",
    ")\n",
    "\n",
    "if RETURN_TIMESTAMP is not None:\n",
    "    pod_scale_down_plot[\"time\"] = scale_down_index.transform(\n",
    "        lambda x: (x - RETURN_TIMESTAMP).total_seconds()\n",
    "    )\n",
    "else:\n",
    "    pod_scale_down_plot[\"time\"] = scale_down_index\n",
    "\n",
    "pod_scale_down_plot = pod_scale_down_plot.set_index(\"time\")\n",
    "\n",
    "\n",
    "# ============= Configure Plot =============\n",
    "\n",
    "# Reorder columns\n",
    "pod_scale_down_plot = pod_scale_down_plot[[\n",
    "    \"cli:rrm\",\n",
    "    # \"hf:released\",\n",
    "    \"pod:delete\",\n",
    "]]\n",
    "\n",
    "pod_scale_down_plot.columns.rename(\"Event Count\", inplace=True)\n",
    "\n",
    "pod_scale_down_plot.rename(\n",
    "    columns={\n",
    "        \"cli:rrm\": \"CLI Request Return Machines\",\n",
    "        \"hf:released\": \"Host Factory API Timestamp: Released\",\n",
    "        \"pod:delete\": \"Pod Deleted\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "if REQUEST_TIMESTAMP is not None:\n",
    "    pod_scale_down_plot.index.rename(\"Time after return API request (seconds)\", inplace=True)\n",
    "else:\n",
    "    pod_scale_down_plot.index.rename(\"Timestamp\", inplace=True)\n",
    "\n",
    "pod_scale_down_fig = pod_scale_down_plot.plot(\n",
    "    kind=\"scatter\",\n",
    "    title=\"Pod Scale Down\"\n",
    ")\n",
    "\n",
    "pod_scale_down_fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0958b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scale_down_fig.write_image(\n",
    "    file=test_folder.joinpath(\"pod_scale_down.svg\"),\n",
    "    format=\"svg\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "pod_scale_down_fig.write_html(\n",
    "    file=test_folder.joinpath(\"pod_scale_down.html\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e14c8e",
   "metadata": {},
   "source": [
    "# Node General Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42438c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = df.copy(deep=True)\n",
    "\n",
    "scale_up_events = dfc[dfc.event == \"scale_up\"].time\n",
    "\n",
    "# Filter pertinent events\n",
    "dfc = dfc[\n",
    "    dfc.event.isin([\n",
    "            \"pod:scheduled\",\n",
    "            \"node:preempted\",\n",
    "            \"node:create\",\n",
    "            \"node:ready_patch\",\n",
    "            \"pod:delete\",\n",
    "            \"node:delete\",\n",
    "            \"pod:create\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "first_node_by_pod = dfc.pivot_table(\n",
    "    index=\"pod\",\n",
    "    values=\"node\",\n",
    "    aggfunc=\"first\"\n",
    ")[\"node\"]\n",
    "\n",
    "# def backprogate_node_in_pod_creation(row):\n",
    "#     row.node = first_node_by_pod.loc[row.pod]\n",
    "#     return row\n",
    "\n",
    "# dfc[dfc.event == \"pod:create\"] = dfc[dfc.event == \"pod:create\"].apply(backprogate_node_in_pod_creation, axis=1)\n",
    "\n",
    "dfc_first = dfc.pivot_table(\n",
    "    index=\"node\",\n",
    "    columns=\"event\",\n",
    "    values=\"time\",\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "dfc = dfc.pivot_table(\n",
    "    index=\"node\",\n",
    "    columns=\"event\",\n",
    "    values=\"time\",\n",
    "    aggfunc=\"last\"\n",
    ").drop(\n",
    "    columns=[\n",
    "        \"pod:scheduled\",\n",
    "    ]\n",
    ").join(dfc_first[[\"pod:scheduled\"]])\n",
    "\n",
    "\n",
    "first_pod_create_timestamp = dfc_pivot[\"first\"][\"pod:create\"].min()\n",
    "first_pod_delete_timestamp = dfc_pivot[\"last\"][\"pod:delete\"].min()\n",
    "\n",
    "# Removes nodes without pods\n",
    "nodes_without_pods = dfc[dfc[\"pod:scheduled\"].isna()].index\n",
    "dfc.drop(index=nodes_without_pods, inplace=True)\n",
    "\n",
    "# # Remove preempted nodes\n",
    "if \"node:preempted\" in dfc.columns:\n",
    "    preempted_nodes = dfc[~dfc[\"node:preempted\"].isna()].index\n",
    "    dfc.drop(index=preempted_nodes, inplace=True)\n",
    "\n",
    "# Calculate delta \n",
    "dfc[\"first-pod-create->node:create\"] = (\n",
    "    dfc[\"node:create\"] - first_pod_create_timestamp\n",
    ").apply(lambda x: x.total_seconds())\n",
    "\n",
    "dfc[\"pod:delete->node:delete\"] = (dfc[\"node:delete\"] - dfc[\"pod:delete\"]).apply(lambda x: x.total_seconds())\n",
    "\n",
    "\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Number of node create events: {(df.event == \"node:create\").sum()}\")\n",
    "print(f\"Number of node preemption events: {(df.event == \"node:preempted\").sum()}\")\n",
    "print(f\"Number of unique nodes preempted: {len(df[df.event == \"node:preempted\"].node.unique())}\")\n",
    "print(f\"Number of unique nodes: {len(df.node.dropna().unique())}\")\n",
    "print(f\"Number of nodes without pods scheduled: {len(nodes_without_pods)}\")\n",
    "\n",
    "dfc_desc = dfc.describe(\n",
    "    percentiles=[\n",
    "        0.25, 0.5, 0.77, 0.99\n",
    "    ]\n",
    ")\n",
    "\n",
    "dfc_desc = dfc_desc.rename(columns={\n",
    "    \"first-pod-create->node:create\": \"First Pod Created to Node Created\",\n",
    "    \"pod:delete->node:delete\": \"First Pod Deleted to Node Deleted\"\n",
    "})\n",
    "\n",
    "dfc_desc\n",
    "\n",
    "# First (overall pod creation) \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.to_csv(\n",
    "    test_folder.joinpath(\"node_events.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbebcb0",
   "metadata": {},
   "source": [
    "# Node Scale Up Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_scale_up = dfc.copy(deep=True)\n",
    "\n",
    "node_scale_up = node_scale_up[[\n",
    "    \"node:create\",\n",
    "    # \"node:preempted\",\n",
    "    \"node:ready_patch\",\n",
    "    \"pod:scheduled\",\n",
    "    # \"pod:create\"\n",
    "]]\n",
    "\n",
    "node_scale_up = node_scale_up.melt().set_index(\"value\").sort_index().reset_index()\n",
    "\n",
    "node_scale_up[\"count\"] = node_scale_up.groupby(\"event\").cumcount()\n",
    "\n",
    "# node_scale_up_index = node_scale_up[\"value\"].apply(\n",
    "#     lambda x:  (x - first_pod_create_timestamp).total_seconds()\n",
    "# )\n",
    "\n",
    "node_scale_up_index = node_scale_up[\"value\"]\n",
    "\n",
    "node_scale_up = node_scale_up.pivot(\n",
    "    columns=\"event\",\n",
    "    values=\"count\"\n",
    ")\n",
    "\n",
    "node_scale_up[\"time\"] = node_scale_up_index\n",
    "node_scale_up = node_scale_up.set_index(\"time\")\n",
    "\n",
    "\n",
    "# =========== Format plot ===========\n",
    "\n",
    "node_scale_up.columns.rename(\"Event Count\", inplace=True)\n",
    "# node_scale_up.index.rename(\"Time after first pod creation (seconds)\", inplace=True)\n",
    "node_scale_up.index.rename(\"Timestamp\", inplace=True)\n",
    "\n",
    "\n",
    "node_scale_up.rename(\n",
    "    columns={\n",
    "        \"node:create\": \"Node Created\",\n",
    "        \"pod:scheduled\": \"First Pod Scheduled\",\n",
    "        \"pod:create\": \"First Pod Created\",\n",
    "        \"node:ready_patch\": \"Node Ready\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "node_scale_up_fig = node_scale_up.plot(\n",
    "    kind=\"scatter\",\n",
    "    title=\"Node Scaleup\",\n",
    ")\n",
    "\n",
    "node_scale_up_fig.update_traces(connectgaps=True)\n",
    "\n",
    "node_scale_up_fig.add_vline(\n",
    "    REQUEST_TIMESTAMP,\n",
    "    line_dash=\"dash\", line_color=\"green\"\n",
    ")\n",
    "\n",
    "# node_scale_up_fig.update_layout(\n",
    "#     plot_bgcolor=\"white\",\n",
    "# )\n",
    "\n",
    "# for scale_up_timestamp in scale_up_events.to_list():\n",
    "#     node_scale_up_fig = node_scale_up_fig.add_vline(\n",
    "#         scale_up_timestamp,\n",
    "#         line_dash=\"dash\", line_color=\"green\"\n",
    "# )\n",
    "\n",
    "node_scale_up_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_scale_up.to_csv(DATA_FOLDER.joinpath(RUN_ID).joinpath(\"node_scale_up.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_scale_up_fig.write_image(\n",
    "    file=test_folder.joinpath(\"node_scale_up.svg\"),\n",
    "    format=\"svg\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "node_scale_up_fig.write_html(\n",
    "    test_folder.joinpath(\"node_scale_up.html\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552086b",
   "metadata": {},
   "source": [
    "# Node Scale Down Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_scale_down = dfc.copy(deep=True)\n",
    "\n",
    "node_scale_down = node_scale_down[[\n",
    "    \"pod:delete\",\n",
    "    \"node:delete\",\n",
    "    # \"node:preempted\"\n",
    "]]\n",
    "\n",
    "node_scale_down = node_scale_down.melt().set_index(\"value\").sort_index().reset_index()\n",
    "\n",
    "node_scale_down[\"count\"] = node_scale_down.groupby(\"event\").cumcount()\n",
    "\n",
    "node_scale_down.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_scale_down_index = node_scale_down[\"value\"].apply(\n",
    "    lambda x:  (x - first_pod_delete_timestamp).total_seconds()\n",
    ")\n",
    "\n",
    "node_scale_down = node_scale_down.pivot(\n",
    "    columns=\"event\",\n",
    "    values=\"count\"\n",
    ")\n",
    "\n",
    "node_scale_down[\"time\"] = node_scale_down_index\n",
    "node_scale_down = node_scale_down.set_index(\"time\")\n",
    "\n",
    "# =========== Format plot ===========\n",
    "\n",
    "node_scale_down.columns.rename(\"Event Count\", inplace=True)\n",
    "node_scale_down.index.rename(\"Time after first intentional pod deletion (seconds)\", inplace=True)\n",
    "\n",
    "node_scale_down = node_scale_down[[\n",
    "    \"pod:delete\",\n",
    "    \"node:delete\"\n",
    "]]\n",
    "\n",
    "node_scale_down.rename(\n",
    "    columns={\n",
    "        \"pod:delete\": \"Last pod deleted\",\n",
    "        \"node:delete\": \"Node deleted\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "node_scale_down_fig = node_scale_down.plot(\n",
    "    kind=\"scatter\",\n",
    "    title=\"Node Scaledown\",\n",
    ")\n",
    "\n",
    "node_scale_down_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.to_csv(\n",
    "    test_folder.joinpath(\"node_timing.csv\")\n",
    ")\n",
    "\n",
    "dfc_desc.to_csv(\n",
    "    test_folder.joinpath(\"node_timing_description.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad31575",
   "metadata": {},
   "source": [
    "# Post Analysis\n",
    "\n",
    "Parse .csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49562bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "\n",
    "\n",
    "cpu_df = pd.read_csv(\n",
    "    test_folder\n",
    "        .joinpath(\"gke_cpus.csv\"),\n",
    "    header=None,\n",
    "    skiprows=2,\n",
    ")\n",
    "\n",
    "cpu_df = cpu_df.rename(columns={\n",
    "    0: \"time\",\n",
    "    1: \"cpus\"\n",
    "})\n",
    "\n",
    "cpu_df.time = cpu_df.time.transform(\n",
    "    lambda x: (parser.parse(x.split(\" (\")[0]) - REQUEST_TIMESTAMP).total_seconds() - 60\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf96daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    test_folder\n",
    "        .joinpath(\"parsed\")\n",
    "        .joinpath(\"node_events.csv\"),\n",
    "    parse_dates=[\n",
    "        \"node:create\",\n",
    "        \"node:delete\",\n",
    "        \"node:ready_patch\",\n",
    "        \"pod:delete\",\n",
    "        \"pod:scheduled\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = df.set_index(\"node\")\n",
    "\n",
    "df[\"node:ready_patch\"] = df[\"node:ready_patch\"].transform(\n",
    "    lambda x: (x - REQUEST_TIMESTAMP).total_seconds()\n",
    ")\n",
    "\n",
    "df[\"pod:scheduled\"] = df[\"pod:scheduled\"].transform(\n",
    "    lambda x: (x - REQUEST_TIMESTAMP).total_seconds()\n",
    ")\n",
    "\n",
    "\n",
    "nodes = df[\"node:ready_patch\"].sort_values().reset_index(drop=True).to_dict()\n",
    "pods = df[\"pod:scheduled\"].sort_values().reset_index(drop=True).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05d2d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(nodes.values()),\n",
    "        y=list(nodes.keys()),\n",
    "        mode=\"lines\",\n",
    "        name=\"GKE - Number of nodes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cpu_df.time,\n",
    "        y=cpu_df.cpus,\n",
    "        mode=\"lines\",\n",
    "        name=\"GKE - Number of cores\"\n",
    "    ),\n",
    "    secondary_y=True\n",
    "   \n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(pods.values()),\n",
    "        y=list(pods.keys()),\n",
    "        mode=\"lines\",\n",
    "        name=\"Number of Symphony Pods\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Scaling performance of IBM Spectrum Symphony connector for GKE\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    xaxis_range=[0,600],\n",
    "    legend=dict(\n",
    "        x=0.005,\n",
    "        y=0.95,\n",
    "        bordercolor='black',\n",
    "        borderwidth=1\n",
    "    )\n",
    "    # yaxis_range=([0,6000],[0,1])\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_xaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey',\n",
    "    tickvals=list(range(60,660,60)),\n",
    "    ticktext=[f\"{x} min\" for x in range(1,11)],\n",
    "    title_text=\"Time After Symphony Request\" \n",
    "    # tickangle=45\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey',\n",
    "    tickcolor=\"lightgrey\",\n",
    "    zerolinecolor='lightgrey',\n",
    "    title_text=\"Number of Pods and Nodes\" \n",
    "\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=False,\n",
    "    showgrid=False,\n",
    "    linecolor=None,\n",
    "    gridcolor=None,\n",
    "    title_text=\"Number of Cores\",\n",
    "    secondary_y=True,   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23c39832",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\n",
    "    file=test_folder.joinpath(\"scale_up.png\"),\n",
    "    format=\"png\",\n",
    "    width=800,\n",
    "    height=500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4c7d7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\n",
    "    test_folder.joinpath(\"scaleup.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eddc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(test_folder.joinpath(\"pods_ready_count.json\"),\"w\") as fh:\n",
    "    json.dump(\n",
    "        pods,\n",
    "        fh\n",
    "    )\n",
    "\n",
    "with open(test_folder.joinpath(\"nodes_ready_count.json\"),\"w\") as fh:\n",
    "    json.dump(\n",
    "        nodes,\n",
    "        fh\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
